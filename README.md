# ğŸ§  Digit Recognizer from Scratch (No TensorFlow)

This project demonstrates how to build a **handwritten digit recognizer** from scratch using only NumPy and basic Python libraries â€” no high-level deep learning frameworks like TensorFlow or PyTorch.

> **Goal:** Classify digits (0â€“9) from the [MNIST dataset](https://www.kaggle.com/c/digit-recognizer/data) using a simple neural network built from the ground up.

---

## ğŸ” Overview

This notebook walks through the full pipeline of creating a neural network, including:

- ğŸ§¹ Data Preprocessing  
- ğŸ—ï¸ Building a feedforward neural network (1 hidden layer)  
- ğŸ“‰ Implementing backpropagation  
- ğŸ§  Training using gradient descent  
- ğŸ“Š Evaluating model accuracy  

---

## ğŸ“ Files

- `digit-recognizer-from-scratch.ipynb` â€” main notebook with all the code and explanations.  
- `train.csv` â€” training data (download from Kaggle).  
- `test.csv` â€” test data (optional: for evaluating model predictions).  

---

## ğŸ§° Dependencies

- Python 3.8+
- NumPy
- Pandas
- Matplotlib (for visualizations)

Install them using:

```bash
pip install numpy pandas matplotlib
```

---

## ğŸš€ How to Run

1. Clone this repository:
   ```bash
   git clone https://github.com/vir123-devf/digit-recognizer-from-scratch.git
   cd digit-recognizer-from-scratch
   ```

2. Open the notebook:
   ```bash
   jupyter notebook digit-recognizer-from-scratch.ipynb
   ```

3. Make sure `train.csv` is in the same folder, or update the path in the notebook.

4. Run all cells to train and evaluate the model.

---

## ğŸ“ˆ Sample Output

- Training Accuracy: ~85â€“92% (depending on hyperparameters)
- No ML libraries used â€” fully custom gradient descent implementation

---

## ğŸ’¡ What Youâ€™ll Learn

- How neural networks work internally  
- How to implement forward and backward propagation manually  
- Basics of gradient descent optimization  
- MNIST digit classification from raw pixel data  

